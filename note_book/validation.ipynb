{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is my code for data_validation\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas.api.types import is_dtype_equal\n",
    "from typing import Dict, Any ,Tuple\n",
    "from evidently.model_profile import Profile\n",
    "from evidently.model_profile.sections import DataDriftProfileSection\n",
    "\n",
    "from src.exception import Cardeo_risk_Exception\n",
    "from src.logger import logging\n",
    "from src.utils.main_utils import read_yaml_file, write_yaml_file\n",
    "from src.entity.artifact_entity import DataIngestionArtifact, DataValidationArtifact\n",
    "from src.entity.config_entity import DataValidationConfig\n",
    "from src.constants import SCHEMA_FILE_PATH\n",
    "\n",
    "\n",
    "class DataValidation:\n",
    "    def __init__(self, data_ingestion_artifact: DataIngestionArtifact, data_validation_config: DataValidationConfig):\n",
    "        \"\"\"\n",
    "        :param data_ingestion_artifact: Output reference of data ingestion artifact stage\n",
    "        :param data_validation_config: configuration for data validation\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.data_ingestion_artifact = data_ingestion_artifact\n",
    "            self.data_validation_config = data_validation_config\n",
    "            self._schema_config =read_yaml_file(file_path=SCHEMA_FILE_PATH)\n",
    "        except Exception as e:\n",
    "            raise Cardeo_risk_Exception(e,sys)\n",
    "\n",
    "    def validate_number_of_columns(self, dataframe: DataFrame) -> bool:\n",
    "        \"\"\"\n",
    "        Method Name :   validate_number_of_columns\n",
    "        Description :   This method validates the number of columns\n",
    "\n",
    "        Output      :   Returns bool value based on validation results\n",
    "        On Failure  :   Write an exception log and then raise an exception\n",
    "        \"\"\"\n",
    "        try:\n",
    "            status = len(dataframe.columns) == len(self._schema_config[\"columns\"])\n",
    "            logging.info(f\"Is required column present: [{status}]\")\n",
    "            return status\n",
    "        except Exception as e:\n",
    "            raise Cardeo_risk_Exception(e, sys)\n",
    "\n",
    "    def is_column_exist(self, df: DataFrame) -> bool:\n",
    "        \"\"\"\n",
    "        Method Name :   is_column_exist\n",
    "        Description :   This method validates the existence of a numerical and categorical columns\n",
    "\n",
    "        Output      :   Returns bool value based on validation results\n",
    "        On Failure  :   Write an exception log and then raise an exception\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dataframe_columns = df.columns\n",
    "            missing_numerical_columns = []\n",
    "            missing_categorical_columns = []\n",
    "            for column in self._schema_config[\"numerical_columns\"]:\n",
    "                if column not in dataframe_columns:\n",
    "                    missing_numerical_columns.append(column)\n",
    "\n",
    "            if len(missing_numerical_columns)>0:\n",
    "                logging.info(f\"Missing numerical column: {missing_numerical_columns}\")\n",
    "\n",
    "\n",
    "            for column in self._schema_config[\"categorical_columns\"]:\n",
    "                if column not in dataframe_columns:\n",
    "                    missing_categorical_columns.append(column)\n",
    "\n",
    "            if len(missing_categorical_columns)>0:\n",
    "                logging.info(f\"Missing categorical column: {missing_categorical_columns}\")\n",
    "\n",
    "            return False if len(missing_categorical_columns)>0 or len(missing_numerical_columns)>0 else True\n",
    "        except Exception as e:\n",
    "            raise Cardeo_risk_Exception(e, sys) from e\n",
    "\n",
    "\n",
    "\n",
    "    def detect_missing_values(self, dataframe: DataFrame) -> bool:\n",
    "        \"\"\"Method Name :   detect_missing_values\n",
    "        Description :   This method validates if there are any missing values in the data\n",
    "        Output      :   Returns bool value based on validation results\n",
    "        On Failure  :   Write an exception log and then raise an exception\n",
    "        \"\"\"\n",
    "        try:\n",
    "            missing_values_status = dataframe.isnull().values.any()\n",
    "            logging.info(f\"Missing values detected: {missing_values_status}\")\n",
    "            return missing_values_status\n",
    "        except Exception as e:\n",
    "            raise Cardeo_risk_Exception(e, sys)\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def read_data(file_path) -> DataFrame:\n",
    "        try:\n",
    "            return pd.read_csv(file_path)\n",
    "        except Exception as e:\n",
    "            raise Cardeo_risk_Exception(e, sys)\n",
    "\n",
    "\n",
    "\n",
    "    def detect_dataset_drift(self, reference_df: DataFrame, current_df: DataFrame, ) -> bool:\n",
    "        \"\"\"\n",
    "        Method Name :   detect_dataset_drift\n",
    "        Description :   This method validates if drift is detected\n",
    "\n",
    "        Output      :   Returns bool value based on validation results\n",
    "        On Failure  :   Write an exception log and then raise an exception\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data_drift_profile = Profile(sections=[DataDriftProfileSection()])\n",
    "\n",
    "            data_drift_profile.calculate(reference_df, current_df)\n",
    "\n",
    "            report = data_drift_profile.json()\n",
    "            json_report = json.loads(report)\n",
    "\n",
    "            write_yaml_file(file_path=self.data_validation_config.drift_report_file_path, content=json_report)\n",
    "\n",
    "            n_features = json_report[\"data_drift\"][\"data\"][\"metrics\"][\"n_features\"]\n",
    "            n_drifted_features = json_report[\"data_drift\"][\"data\"][\"metrics\"][\"n_drifted_features\"]\n",
    "\n",
    "            logging.info(f\"{n_drifted_features}/{n_features} drift detected.\")\n",
    "            drift_status = json_report[\"data_drift\"][\"data\"][\"metrics\"][\"dataset_drift\"]\n",
    "            return drift_status\n",
    "        except Exception as e:\n",
    "            raise Cardeo_risk_Exception(e, sys) from e\n",
    "\n",
    "\n",
    "\n",
    "    def detect_duplicates(self, dataframe: DataFrame) -> bool:\n",
    "        \"\"\"Method Name :   detect_duplicates\n",
    "        Description :   This method validates if there are any duplicate values in the data\n",
    "        Output      :   Returns bool value based on validation results\n",
    "        On Failure  :   Write an exception log and then raise an exception\n",
    "        \"\"\"\n",
    "        try:\n",
    "            duplicate_values_status = dataframe.duplicated().any()\n",
    "            logging.info(f\"Duplicate values detected: {duplicate_values_status}\")\n",
    "            return duplicate_values_status\n",
    "        except Exception as e:\n",
    "            raise Cardeo_risk_Exception(e, sys)\n",
    "\n",
    "    def check_data_types(self, dataframe: DataFrame) -> bool:\n",
    "        \"\"\"\n",
    "        Method Name :   check_data_types\n",
    "        Description :   This method validates if the data types of the columns match the schema\n",
    "        Output      :   Returns bool value based on validation results\n",
    "        On Failure  :   Write an exception log and then raise an exception\n",
    "        \"\"\"\n",
    "        try:\n",
    "            schema_col_dtype = self._schema_config.get(\"dtypes\",{})\n",
    "            schema_dtype=list(schema_col_dtype.values())\n",
    "            schema_col=list(schema_col_dtype.keys())\n",
    "            df_dtype=[dtype.name for dtype in dataframe.dtypes]\n",
    "            df_col=list(dataframe.columns)\n",
    "\n",
    "            mismatched_columns = []\n",
    "\n",
    "            for column in schema_col:\n",
    "                if column in df_col:\n",
    "                    logging.info(f\"schema column: {column}  is present in dataframe\")\n",
    "\n",
    "                else:\n",
    "                    mismatched_columns.append(f\"{column} is missing in the DataFrame.\")\n",
    "                    logging.info(f\"Column not found in dataframe: {column}\")\n",
    "                    continue\n",
    "\n",
    "            mismatched_dtypes = []\n",
    "\n",
    "            for dtype in schema_dtype:\n",
    "                if dtype  in df_dtype:\n",
    "                    logging.info(f\"schema dtype : {dtype}  is present in dataframe dtype\")\n",
    "                else:\n",
    "                    mismatched_dtypes.append(f\"Data type mismatch for column: {column} , Found: {dtype}\")\n",
    "\n",
    "            if len(mismatched_columns)>0:\n",
    "                logging.info(f\"schema data column mismatched with dataframe_columns.\")\n",
    "\n",
    "            if len(mismatched_dtypes)>0:\n",
    "                logging.info(f\"schema data type mismatched with dataframe_dtypes.\")\n",
    "\n",
    "            return False if len(mismatched_columns)>0 or len(mismatched_dtypes)>0 else True\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Cardeo_risk_Exception(e, sys)\n",
    "\n",
    "\n",
    "\n",
    "    def rename_columns(self, dataframe: DataFrame) -> bool:\n",
    "        \"\"\"\n",
    "        Method Name :   rename_columns\n",
    "        Description :   Renames columns based on the schema's column_rename section.\n",
    "        Output      :   Returns True if column renaming is successful, False otherwise.\n",
    "        On Failure  :   Logs the exception and raises it.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Access column_rename mapping from schema\n",
    "            renamed_columns = self._schema_config.get(\"column_rename\", {})\n",
    "            if not renamed_columns:\n",
    "                logging.info(\"No columns to rename as per schema configuration.\")\n",
    "                return True\n",
    "\n",
    "\n",
    "            # Rename columns in the dataframe\n",
    "            dataframe.rename(columns=renamed_columns, inplace=True)\n",
    "            logging.info(f\"Columns successfully renamed: {renamed_columns}.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            raise Cardeo_risk_Exception(e, sys)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def initiate_data_validation(self) -> DataValidationArtifact:\n",
    "        \"\"\"\n",
    "        Method Name :   initiate_data_validation\n",
    "        Description :   This method initiates the data validation component for the pipeline\n",
    "\n",
    "        Output      :   Returns bool value based on validation results\n",
    "        On Failure  :   Write an exception log and then raise an exception\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            validation_error_msg = \"\"\n",
    "            logging.info(\"Starting data validation\")\n",
    "            train_df, test_df = (DataValidation.read_data(file_path=self.data_ingestion_artifact.trained_file_path),\n",
    "                                 DataValidation.read_data(file_path=self.data_ingestion_artifact.test_file_path))\n",
    "\n",
    "            status = self.validate_number_of_columns(dataframe=train_df)\n",
    "            logging.info(f\"All required columns present in training dataframe: {status}\")\n",
    "            if not status:\n",
    "                validation_error_msg += f\"Columns are missing in training dataframe.\"\n",
    "            status = self.validate_number_of_columns(dataframe=test_df)\n",
    "\n",
    "            logging.info(f\"All required columns present in testing dataframe: {status}\")\n",
    "            if not status:\n",
    "                validation_error_msg += f\"Columns are missing in test dataframe.\"\n",
    "\n",
    "            status = self.is_column_exist(df=train_df)\n",
    "\n",
    "            if not status:\n",
    "                validation_error_msg += f\"Columns are missing in training dataframe.\"\n",
    "            status = self.is_column_exist(df=test_df)\n",
    "\n",
    "            if not status:\n",
    "                validation_error_msg += f\"columns are missing in test dataframe.\"\n",
    "\n",
    "            status = self.detect_missing_values(dataframe=train_df)\n",
    "            if status:\n",
    "                validation_error_msg += \"Missing values detected in training dataframe.\"\n",
    "\n",
    "\n",
    "            status = self.detect_missing_values(dataframe=test_df)\n",
    "            if status:\n",
    "                validation_error_msg += \"Missing values detected in test dataframe.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            validation_status = len(validation_error_msg) == 0\n",
    "\n",
    "            if validation_status:\n",
    "                drift_status = self.detect_dataset_drift(train_df, test_df)\n",
    "                if drift_status:\n",
    "                    logging.info(f\"Drift detected.\")\n",
    "                    validation_error_msg = \"Drift detected\"\n",
    "                else:\n",
    "                    validation_error_msg = \"Drift not detected in the dataset.\"\n",
    "            else:\n",
    "                logging.info(f\"Validation_error: {validation_error_msg}\")\n",
    "\n",
    "\n",
    "            status = self.detect_duplicates(dataframe=train_df)\n",
    "            if status:\n",
    "                validation_error_msg += \" Duplicates detected in training dataframe.\"\n",
    "\n",
    "\n",
    "            status = self.detect_duplicates(dataframe=test_df)\n",
    "            if status:\n",
    "                validation_error_msg += \"Duplicates detected in test dataframe.\"\n",
    "\n",
    "            status = self.check_data_types(dataframe=train_df)\n",
    "            logging.info(f\"all Columns are data type matched in training dataframe: {status}\")\n",
    "            if not status:\n",
    "                validation_error_msg += \"Data type mismatch in training dataframe.\"\n",
    "\n",
    "\n",
    "            status = self.check_data_types(dataframe=test_df)\n",
    "            logging.info(f\"all Columns are  data type matched in testing dataframe: {status}\")\n",
    "            if not status:\n",
    "                validation_error_msg += \"Data type mismatch in test dataframe.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "             #Rename columns in training dataframe\n",
    "            status = self.rename_columns(dataframe=train_df)\n",
    "            logging.info(f\"Columns renamed in training dataframe: {status}\")\n",
    "            if not status:\n",
    "                validation_error_msg += \"Column renaming failed in training dataframe.\"\n",
    "\n",
    "           # Rename columns in testing dataframe\n",
    "            status = self.rename_columns(dataframe=test_df)\n",
    "            logging.info(f\"Columns renamed in testing dataframe: {status}\")\n",
    "            if not status:\n",
    "                validation_error_msg += \"Column renaming failed in testing dataframe.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            data_validation_artifact = DataValidationArtifact(\n",
    "                validation_status=validation_status,\n",
    "                message=validation_error_msg,\n",
    "                drift_report_file_path=self.data_validation_config.drift_report_file_path\n",
    "            )\n",
    "\n",
    "            logging.info(f\"Data validation artifact: {data_validation_artifact}\")\n",
    "            return data_validation_artifact\n",
    "        except Exception as e:\n",
    "            raise  Cardeo_risk_Exception(e, sys) from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(filename=\"hello.log\", level=logging.DEBUG)\n",
    "\n",
    "\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "\n",
    "a = 1\n",
    "b = 29\n",
    "add_result = add(a, b)\n",
    "logging.debug(\"Add : {} + {} = {}\".format(a, b, add_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "log = logging.getLogger(\"myapp\")\n",
    "log.warning(\"woot\")\n",
    "logging.basicConfig()\n",
    "log.warning(\"woot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
